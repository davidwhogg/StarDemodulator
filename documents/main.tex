\documentclass[12pt]{article}
\usepackage{microtype}
\usepackage[letterpaper]{geometry}
\usepackage{amsmath}
\usepackage[scr=rsfs]{mathalpha}

\addtolength{\topmargin}{-0.60in}
\addtolength{\textheight}{1.70in}
\sloppy\sloppypar\raggedbottom\frenchspacing

\title{\bfseries A noise model for the Phi-M radio}
\author{Hogg}
\date{Revision 0.2 --- 31 August 2023}

\sloppy\sloppypar\raggedbottom\frenchspacing
\begin{document}

\maketitle

\noindent
We have $N$ flux measurements $y_n$ at $N$ known times $t_n$ (in barycentric time).
Each flux measurement $y_n$ has an uncertainty $\sigma_n$.
We will assume that this uncertainty is generated by Gaussian noise with zero mean and correctly estimated variances $\sigma_n^2$

In---or really, generating---this dataset are $K$ pure, coherent sinusoidal signals with amplitudes $A_k$ and frequencies $f_k$, such that we can approximate the data as
\begin{align}
    y_n &= \sum_{k=1}^K A_k\,\cos(2\pi f_k\,[t_n - \Delta_n] + \phi_k) + A_0 + \mbox{noise} ~,\label{eq:model}
\end{align}
where at each time $t_n$ there is a time delay $\Delta_n$ from stellar displacement induced by a companion, the $\phi_k$ are (physically irrelevant but important) mode phases, $A_0$ represents the DC offset of the signal, and the word ``noise'' stands in for everything zero-mean that the model isn't capturing.
In detail, each $\Delta_n$ is a function of companion orbital parameters and the time $t_n$; it represents the displacement of the star from the system center-of-mass position, projected onto the line of sight, converted to time units.
With this sign convention ($t_n - \Delta_n$ in the argument of the cosine), this time-delay quantity $\Delta_n$ can be called called O$-$C (``observed minus computed'') as it is in, for example, transit-timing studies.

We are going to make high-level assumptions like
\begin{itemize}
  \item The modes $f_k$ are high frequency in the relevant sense, or
  \begin{align}
      f_k &\gg \frac{1}{\max_n t_n - \min_n t_n} ~.
  \end{align}
  This ensures that it is possible to know the mode frequencies accurately.
  \item The modes $f_k$ are coherent in the relevant sense, or
  \begin{align}
      W_k &\ll \frac{1}{\max_n t_n - \min_n t_n} ~,
  \end{align}
  where $W_k$ is the intrinsic (infinite-data) width of the mode in frequency units.
  This ensures that there is a well-defined amplitude $A_k$ and phase $\phi_k$ for each mode, independent of time.
  \item Every mode amplitude $A_k$, frequency $f_k$, and phase $\phi_k$ is accurately known (this assumption relies on the previous two).
  \item Every time delay $\Delta_n$ is small in the sense of
  \begin{align}
      |2\pi f_k\,\Delta_n| &\ll 1 ~,
  \end{align}
  so we are safely in the small-angle approximation at all epochs.
  \item There isn't a huge dynamic range in the inverse variances $\sigma_n^{-2}$, such that all data points are fairly typically useful.
\end{itemize}

In the small-angle approximation, the fundamental model \eqref{eq:model} becomes linear in the time delay:
\begin{align}
    \delta y_n &= \Delta_n\,\sum_{k=1}^K 2\pi f_k\,A_k\,\sin(2\pi f_k\,t_n + \phi_k) + \mbox{noise}\label{eq:linear} \\
    \delta y_n &\equiv y_n - A_0 - \sum_{k=1}^K A_k\,\cos(2\pi f_k\,t_n + \phi_k) ~,
\end{align}
where we have defined a residual $\delta y_n$, which is the difference between the lightcurve signal and the sum of pure cosine modes that would explain it in the absence of any time delays.
In linear problems like this, matched filters find signals.
Thus, for each mode $k$, at every data point $n$, we can get a rough time-delay estimate $\hat\Delta_{kn}$ with the following operation:
\begin{align}
    \hat\Delta_{kn} &= \delta y_n\,\frac{2}{2\pi f_k\,A_k}\,\sin(2\pi f_k\,t_n + \phi_k) ~.\label{eq:onemodeestimator}
\end{align}
Actually, this is only a good estimate of the individual-mode time delay when averaged over an integer number of half-periods of the mode $f_k$.
A better estimate is something like
\begin{align}
    \hat\Delta_{kn} &= \delta y_n\,\frac{1}{2\pi f_k\,A_k\,\sin(2\pi f_k\,t_n + \phi_k)} ~,\label{eq:unstable}
\end{align}
but this involves division by something that frequently crosses zero, so it is unstable.
Nonetheless, the rough, stable, single-mode estimator \eqref{eq:onemodeestimator} is useful for rapid discovery of companions, and rapid confirmation that a companion is independently visible in multiple individual modes.

The best estimate of a single-epoch time delay $\Delta_n$, in the context of the linearized model \eqref{eq:model} would make use of all modes simultaneously; it would be
\begin{align}
    \hat\Delta_n &= \delta y_n\,\left[\sum_{k=1}^K 2\pi f_k\,A_k\,\sin(2\pi f_k\,t_n + \phi_k)\right]^{-1} ~,\label{eq:allmodeestimator}
\end{align}
which is the multi-mode generalization of \eqref{eq:unstable}, but which is more stable because the sum makes zero-crossings far less common.
That's cool!
But what if we want to do weighted least squares fitting or maximum likelihood or something Bayesian involving priors?
There are (at least) three options:
\begin{itemize}
    \item Go back to the model \eqref{eq:model} and turn it into a likelihood function. That's Hey.
    \item Estimate time delays $\hat\Delta_n$ according to \eqref{eq:allmodeestimator} and define a chi-squared using the standard uncertainty estimates
    \begin{align}
        \sigma_{\Delta n} &= \sigma_n\,\left[\sum_{k=1}^K 2\pi f_k\,A_k\,\sin(2\pi f_k\,t_n + \phi_k)\right]^{-1} ~.
    \end{align}
    That is, chi-squared becomes something like
    \begin{align}
        \chi^2(\Omega) &= \sum_{n=1}^N \frac{1}{\sigma_{\Delta n}^2}\left[\hat\Delta_n - T(t_n;\Omega)\right]^2 ~,\label{eq:chisquared1}
    \end{align}
    where $\Omega$ are parameters of the orbital companion orbit, and $T(t;\Omega)$ is the predicted time delay induced in the host star by that orbital companion at time $t$.
    \item Go back to the linearized model \eqref{eq:linear} and turn it into a chi-squared like this:
    \begin{align}
        \chi^2(\Omega) &= \sum_{n=1}^N \frac{1}{\sigma_n^2}\left[\delta y_n - T(t_n;\Omega)\, \sum_{k=1}^K 2\pi f_k\,A_k\,\sin(2\pi f_k\,t_n + \phi_k)\right]^2 ~,\label{eq:chisquared2}
    \end{align}
    which just makes use of the residual lightcurve points $\delta y_n$ and their standard errors $\sigma_n$.
    This is the small-angle approximation equivalent of Hey.
\end{itemize}
Some notes on the above:
\textsl{(First note)} Actually, I lied: You can show that the two options for $\chi^2$ \eqref{eq:chisquared1} and \eqref{eq:chisquared2} are numerically identical.

\textsl{(Second note)}
Chi-squared $\chi^2$ is related to the log likelihood $\ln\mathscr{L}$ by
\begin{align}
    \ln\mathscr{L}(\Omega) &= C - \frac{1}{2}\,\chi^2(\Omega) ~,
\end{align}
where $C$ is an offset that doesn't depend on the parameters $\Omega$.

\textsl{(Third note)}
One might want to inflate the uncertainties (that is, replace $\sigma_n^2$ with $\sigma_n^2 + s^2$ for some $s^2$) to account for the signals (variabilities) that are not captured by the $K$ coherent modes.
This leads to two changes.
The first is that chi-squared becomes
\begin{align}
    \chi^2(\Omega,s) &= \sum_{n=1}^N \frac{1}{\sigma_n^2 + s^2}\left[\delta y_n - T(t_n;\Omega)\, \sum_{k=1}^K 2\pi f_k\,A_k\,\sin(2\pi f_k\,t_n + \phi_k)\right]^2 ~,\label{eq:chisquared3}
\end{align}
and the second is that the constant $C$ in the log likelihood will now depend on $s^2$ such that the log likelihood becomes
\begin{align}
    \ln\mathscr{L}(\Omega,s) &= -\sum_{n=1}^N \ln(2\pi\,[\sigma_n^2 + s^2]) - \frac{1}{2}\,\chi^2(\Omega,s) ~,
\end{align}

\end{document}
